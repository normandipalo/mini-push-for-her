{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mini-push-her.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/normandipalo/mini-push-for-her/blob/master/mini_push_her.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2vf3MCm1fW0C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0jsR761S9gje",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create the environment. \n",
        "An agent pushes a block in a grid-world, and must reach a speicfied position with only sparse rewards."
      ]
    },
    {
      "metadata": {
        "id": "wDA7bN5rfZw3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class World():\n",
        "    def __init__(self):\n",
        "        self.map = np.zeros((5,5))\n",
        "        self.observation_space = self.observation()\n",
        "        self.action_space = self.action_sp()\n",
        "        self.agent = np.array([0.,0.])\n",
        "        self.cube = np.array([2.,2.])\n",
        "        self.goal = np.array([3.,3.])\n",
        "        \n",
        "    def reset(self):\n",
        "        self.map = np.zeros((5,5))\n",
        "        self.agent = np.random.randint(0, 4, size = (2)).reshape((2))\n",
        "        self.agent = np.array(self.agent, dtype = np.float32)\n",
        "        self.cube = np.random.randint(1, 3, size = (2)).reshape((2))\n",
        "        self.cube = np.array(self.cube, dtype = np.float32)\n",
        "        while (self.agent == self.cube).all():\n",
        "            self.cube = np.random.randint(1, 3, size = (2)).reshape((2))\n",
        "            self.cube = np.array(self.cube, dtype = np.float32)\n",
        "            \n",
        "        self.goal = np.random.randint(0, 4, size = (2)).reshape((2))\n",
        "        self.goal = np.array(self.cube, dtype = np.float32)\n",
        "        while (self.goal == self.cube).all():\n",
        "            self.goal = np.random.randint(0, 4, size = (2)).reshape((2))\n",
        "            self.goal = np.array(self.goal, dtype = np.float32)\n",
        "            \n",
        "        return np.concatenate((self.agent, self.cube, self.goal), axis = 0)\n",
        "    \n",
        "    def _move_agent(self, action):\n",
        "        self.agent = self.agent + np.array(action)\n",
        "        self.agent = np.clip(self.agent, 0., 4.)\n",
        "    \n",
        "    def step(self, action):\n",
        "        old_agent = self.agent.copy()\n",
        "        self._move_agent(action)\n",
        "        new_agent = self.agent.copy()\n",
        "        if (new_agent == self.cube).all():\n",
        "            delta = new_agent - old_agent\n",
        "            delta = np.array(delta, dtype = np.float32)\n",
        "            self.cube+=delta\n",
        "        reward = -1.\n",
        "        done = False\n",
        "        if (self.cube == self.goal).all():\n",
        "            reward = 1.\n",
        "            done = True\n",
        "        \n",
        "        return np.concatenate((self.agent, self.cube, self.goal), axis = 0), reward, done, {\"hello\": \"world\"}\n",
        "    \n",
        "    def show(self):\n",
        "        mappa = self.map.copy()\n",
        "        mappa[int(self.agent[0]), int(self.agent[1])] = 1.\n",
        "        mappa[int(np.clip(self.cube[0], 0, 4)), int(np.clip(self.cube[1], 0, 4))] = 2.\n",
        "        mappa[int(np.clip(self.goal[0], 0, 4)), int(np.clip(self.goal[1], 0, 4))] = 3.\n",
        "        print(mappa)\n",
        "        \n",
        "    class observation():\n",
        "        def __init__(self):\n",
        "            self.shape = [6]\n",
        "\n",
        "    class action_sp():\n",
        "        def __init__(self):\n",
        "            self.shape = [2]\n",
        "\n",
        "        def sample(self):\n",
        "            act = np.array([np.random.randint(-1, 2), np.random.randint(-1, 2)], dtype = np.float32)\n",
        "            return act"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D4UE_hbafcM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vUrqjhkepfus",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_trans(state, action, next_state):\n",
        "    if np.linalg.norm((next_state[2:4] - next_state[-2:])) < 0.1:\n",
        "        done = True\n",
        "        reward = 1.\n",
        "    else:\n",
        "        done = False\n",
        "        reward = -1.\n",
        "    return reward, done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S_KmnSH39skw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep Deterministic Policy Gradient.\n",
        "Create the various classes and functions needed to perform DDPG, such as Policy and Value Networks, Target Networks, Replay Buffer etc."
      ]
    },
    {
      "metadata": {
        "id": "wTUy70WlffQg",
        "colab_type": "code",
        "outputId": "d2acbac2-471b-4f2f-ef34-1f73b9c73ad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "      \n",
        "class ValueNetwork(tf.keras.Model):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_size, init_w = 3e-3):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        \n",
        "        self.linear1 = tf.keras.layers.Dense(units = hidden_size, activation = \"relu\")\n",
        "        self.linear2 = tf.keras.layers.Dense(units = hidden_size, activation = \"relu\")\n",
        "        self.linear3 = tf.keras.layers.Dense(units = 1, kernel_initializer = tf.keras.initializers.RandomUniform(minval = -init_w, maxval = init_w), bias_initializer = tf.keras.initializers.RandomUniform(minval = -init_w, maxval = init_w))\n",
        "        \n",
        "    def call(self, state, action):\n",
        "        action = tf.convert_to_tensor(action) #serve?\n",
        "        state = tf.convert_to_tensor(state)\n",
        "        x = tf.concat((action, state), axis = 1)\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "    \n",
        "class PolicyNetwork(tf.keras.Model):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_size, init_w = 3e-3):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        \n",
        "        self.linear1 = tf.keras.layers.Dense(units = hidden_size, activation = \"relu\")\n",
        "        self.linear2 = tf.keras.layers.Dense(units = hidden_size, activation = \"relu\")\n",
        "        self.linear3 = tf.keras.layers.Dense(units = num_actions, activation = \"tanh\", kernel_initializer = tf.keras.initializers.RandomUniform(minval = -init_w, maxval = init_w),  bias_initializer = tf.keras.initializers.RandomUniform(minval = -init_w, maxval = init_w))\n",
        "        \n",
        "    def call(self, state):\n",
        "        state = tf.convert_to_tensor(state)\n",
        "        x = self.linear1(state)\n",
        "        x = self.linear2(x)\n",
        "        x = self.linear3(x)\n",
        "        x = x*1\n",
        "        return x\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state = tf.convert_to_tensor(state)\n",
        "        state = tf.expand_dims(state, 0)\n",
        "        action = self.call(state)\n",
        "        return action.numpy().squeeze(0) #detach -> stop gradients?\n",
        "      \n",
        "pol_losses = []\n",
        "val_losses = []\n",
        "def ddpg_update(batch_size, gamma = 0.9, min_value = -np.inf,\n",
        "                max_value = np.inf, soft_tau1 = 1e-1, soft_tau2 = 1e0, replay_buffer = None):\n",
        "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
        "    \n",
        "    PRINT = False\n",
        "    #al posto di expand dims potrei quando possibile expandere il numpy con[None,:]\n",
        "    state = tf.convert_to_tensor(state, dtype = tf.float64)\n",
        "    if PRINT: print(\"state shape\", state.numpy().shape)\n",
        "    next_state = tf.convert_to_tensor(next_state, dtype = tf.float64)\n",
        "    if PRINT: print(\"next state shape\", next_state.numpy().shape)\n",
        "    action = tf.convert_to_tensor(action, dtype = tf.float64)\n",
        "    if PRINT: print(\"action shape\", action.numpy().shape)\n",
        "    reward = tf.expand_dims(tf.convert_to_tensor(reward, dtype = tf.float64),1)\n",
        "    if PRINT: print(\"reward shape\", reward.numpy().shape)\n",
        "    done = tf.expand_dims(tf.convert_to_tensor(np.float32(done), dtype = tf.float64), axis = 1)\n",
        "    \n",
        "    if PRINT: print(\"done shape\", done.numpy().shape)\n",
        "    with tf.GradientTape() as tape:\n",
        "        policy_loss = value_net(state, policy_net(state))\n",
        "        policy_loss = -tf.reduce_mean(policy_loss)\n",
        "        pol_losses.append(policy_loss)\n",
        "        if PRINT: print(\"Policy Loss\", policy_loss)\n",
        "        policy_grads = tape.gradient(policy_loss, policy_net.variables)\n",
        "        \n",
        "    #    print(\"Policy Grads\", [tf.reduce_mean(policy_grad).numpy() for policy_grad in policy_grads]) \n",
        "        policy_optimizer.apply_gradients(zip(policy_grads, policy_net.variables),\n",
        "                            global_step=tf.train.get_or_create_global_step())\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "        #rimetti target\n",
        "        next_action = target_policy_net(next_state)\n",
        "        target_value = target_value_net(next_state, tf.stop_gradient(next_action)) #next action è un tensore che arriva\n",
        "        expected_value = reward + (1.0 - done)*gamma*target_value             #da una rete quindi per questo dobbiamo impedire che venga backprop\n",
        "        \n",
        "        \n",
        "        value = value_net(state, action)\n",
        "        if PRINT: print(\"value shape\", value.numpy().shape)\n",
        "        if PRINT: print(\"exp value shape\", expected_value.numpy().shape)\n",
        "        value_loss = tf.losses.mean_squared_error(tf.stop_gradient(expected_value), value)\n",
        "        val_losses.append(value_loss)\n",
        "   #     value_loss = tf.reduce_mean(tf.square((tf.stop_gradient(expected_value) - value)))\n",
        "        if PRINT: print(\"Value Loss\", value_loss)\n",
        "\n",
        "        value_grads = tape.gradient(value_loss, value_net.variables)\n",
        "        value_optimizer.apply_gradients(zip(value_grads, value_net.variables),\n",
        "                            global_step=tf.train.get_or_create_global_step())\n",
        "    \n",
        "    for target_param, param in zip(target_value_net.variables, value_net.variables):\n",
        "        tf.assign(target_param, (target_param*(1.0 - soft_tau1) + param * soft_tau1))\n",
        "        \n",
        "    for target_param, param in zip(target_policy_net.variables, policy_net.variables):\n",
        "        tf.assign(target_param, (target_param*(1.0 - soft_tau1) + param * soft_tau1))\n",
        "   \n",
        "    for target_param, param in zip(target2_value_net.variables, target_value_net.variables):\n",
        "        tf.assign(target_param, (target_param*(1.0 - soft_tau2) + param * soft_tau2))\n",
        "        \n",
        "    for target_param, param in zip(target2_policy_net.variables, target_policy_net.variables):\n",
        "        tf.assign(target_param, (target_param*(1.0 - soft_tau2) + param * soft_tau2))\n",
        "        \n",
        "N = False\n",
        "env = World()\n",
        "print(env.action_space.shape)\n",
        "\n",
        "\n",
        "state_dim  = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]  if not N else env.action_space.n\n",
        "hidden_dim = 50\n",
        "\n",
        "value_net  = ValueNetwork(state_dim, action_dim, hidden_dim)\n",
        "policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
        "\n",
        "#Actually initialize network, otherwise it has no weights \n",
        "value_net(tf.convert_to_tensor(np.random.randn(1,state_dim)), action = tf.convert_to_tensor(np.random.randn(1,action_dim)))\n",
        "policy_net(tf.convert_to_tensor(np.random.randn(1, state_dim)))\n",
        "\n",
        "target_value_net  = ValueNetwork(state_dim, action_dim, hidden_dim)\n",
        "target_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
        "\n",
        "#Actually initialize network, otherwise it has no weights\n",
        "target_value_net(tf.convert_to_tensor(np.random.randn(1,state_dim)), action = tf.convert_to_tensor(np.random.randn(1,action_dim)))\n",
        "target_policy_net(tf.convert_to_tensor(np.random.randn(1, state_dim)))\n",
        "\n",
        "\n",
        "for target_param, param in zip(target_value_net.variables, value_net.variables):\n",
        "    tf.assign(target_param , param)\n",
        "\n",
        "for target_param, param in zip(target_policy_net.variables, policy_net.variables):\n",
        "    tf.assign(target_param , param)\n",
        "    \n",
        "target2_value_net  = ValueNetwork(state_dim, action_dim, hidden_dim)\n",
        "target2_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
        "\n",
        "#Actually initialize network, otherwise it has no weights\n",
        "target2_value_net(tf.convert_to_tensor(np.random.randn(1,state_dim)), action = tf.convert_to_tensor(np.random.randn(1,action_dim)))\n",
        "target2_policy_net(tf.convert_to_tensor(np.random.randn(1, state_dim)))\n",
        "\n",
        "for target_param, param in zip(target2_value_net.variables, value_net.variables):\n",
        "    tf.assign(target_param , param)\n",
        "\n",
        "for target_param, param in zip(target2_policy_net.variables, policy_net.variables):\n",
        "    tf.assign(target_param , param)\n",
        "\n",
        "value_lr  = 1e-3\n",
        "policy_lr = 1e-4\n",
        "\n",
        "value_optimizer = tf.train.AdamOptimizer(learning_rate = value_lr)\n",
        "policy_optimizer = tf.train.AdamOptimizer(learning_rate = policy_lr)\n",
        "\n",
        "replay_buffer_size = 1000000\n",
        "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R9ZsxD0Q93MP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hindsight Experience Replay.\n",
        "Define the HER algorithm, which takes an experienced trajectory, stores it into buffer and also stores a modified trajectory substituting the goal with future reached stated."
      ]
    },
    {
      "metadata": {
        "id": "EN0wiQI2foFU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def herify(states, actions, rewards, next_states, dones, buffer):\n",
        "        T = len(states)\n",
        "        for t in range(T-1):\n",
        "            buffer.push(states[t].copy(), actions[t].copy(), rewards[t], next_states[t].copy(), dones[t])\n",
        "            for k in range(4):\n",
        "                future_step = np.random.randint(t+1,T)\n",
        "                future_goal = next_states[future_step][2:4]\n",
        "                state_f = deepcopy(states[t])\n",
        "                \n",
        "                if np.linalg.norm((state_f[2:4] - future_goal)) < 0.1: #se il goal fututo è già realizzato skippa\n",
        "                    continue\n",
        "                \n",
        "                state_f[-2:] = future_goal\n",
        "                next_state_f = deepcopy(next_states[t])\n",
        "                next_state_f[-2:] = future_goal\n",
        "                r_f, d_f = evaluate_trans(state_f, actions[t], next_state_f)\n",
        "                buffer.push(state_f, actions[t], r_f, next_state_f, d_f)\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RdktdxV0foaM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "replay_buffer = ReplayBuffer(100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0axgW6IYftDC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ev_policy():\n",
        "    fails, wins = 0,0\n",
        "    for i in range(100):\n",
        "        env = World()\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        st = 0\n",
        "        while not done:\n",
        "            st+=1\n",
        "            if st == 30: \n",
        "                done = True\n",
        "                fails+=1\n",
        "                break\n",
        "            act = policy_net.get_action(np.array(state, dtype = np.float64))\n",
        "            act = np.array(act, dtype = np.float64)\n",
        "            state = np.array(state, dtype = np.float64)\n",
        "       #     print(\"value\", value_net(state.reshape((1,-1)), act.reshape((1,-1))))\n",
        "            act += np.random.randn(2)*0.1\n",
        "        #    print(act)\n",
        "            for a in range(len(act)):\n",
        "                if act[a] < -0.3 : act[a] = -1.\n",
        "                if act[a] > -0.3 and act[a] < 0.3: act[a] = 0.\n",
        "                if act[a] > 0.3: act[a] = 1.\n",
        "        #    print(act)\n",
        "            next_state, reward, done, _ = env.step(act)\n",
        "            if reward==1:\n",
        "                wins+=1\n",
        "        #    print(next_state)\n",
        "        #    env.show()\n",
        "            state = next_state\n",
        "        #    print(\"REWARD\", reward, \"DONE\", done)\n",
        "    return wins"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNGlfz-f-FgU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training phase."
      ]
    },
    {
      "metadata": {
        "id": "_dhPKYDRfuau",
        "colab_type": "code",
        "outputId": "f5c72a2e-365f-489a-82b6-31c41ca644d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1552
        }
      },
      "cell_type": "code",
      "source": [
        "#generate rollout\n",
        "winslist = []\n",
        "states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "env = World()\n",
        "for i in range(500000):\n",
        "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        steps += 1\n",
        "        act =policy_net.get_action(np.array(state, dtype = np.float64))\n",
        "        act += np.random.randn(2)*0.1\n",
        "        act_disc = deepcopy(act)\n",
        "        for a in range(len(act)):\n",
        "            if act[a] < -0.3 : act_disc[a] = -1.\n",
        "            if act[a] > -0.3 and act[a] < 0.3: act_disc[a] = 0.\n",
        "            if act[a] > 0.3: act_disc[a] = 1.\n",
        "        next_state, reward, done, _ = env.step(act_disc)\n",
        "        if steps > 30: break\n",
        "        states.append(state)\n",
        "        actions.append(act)\n",
        "        rewards.append(reward)\n",
        "        next_states.append(next_state)\n",
        "        dones.append(done)\n",
        "        state = next_state\n",
        "    herify(states, actions, rewards, next_states, dones, replay_buffer)\n",
        "    if i % 10 == 0 and not i == 0:\n",
        "        for k in range(10):\n",
        "            if len(replay_buffer) > 128:\n",
        "              ddpg_update(64, replay_buffer=replay_buffer)\n",
        "    if i % 501 == 0:\n",
        "        wins = 0\n",
        "        \n",
        "        wins+=ev_policy()\n",
        "        winslist.append(wins)\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(20,5))\n",
        "        plt.subplot(131)\n",
        "        plt.plot(winslist)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEvCAYAAABouFsBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f775e2032e8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a273b21aa830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m               \u001b[0mddpg_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m501\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mwins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-54ae1ea1fd12>\u001b[0m in \u001b[0;36mddpg_update\u001b[0;34m(batch_size, gamma, min_value, max_value, soft_tau1, soft_tau2, replay_buffer)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mvalue_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         value_optimizer.apply_gradients(zip(value_grads, value_net.variables),\n\u001b[0;32m--> 108\u001b[0;31m                             global_step=tf.train.get_or_create_global_step())\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtarget_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_value_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    608\u001b[0m           \u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m           \u001b[0mupdate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mapply_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mupdate_op\u001b[0;34m(self, optimizer, g)\u001b[0m\n\u001b[1;32m    165\u001b[0m       return optimizer._resource_apply_sparse_duplicate_indices(\n\u001b[1;32m    166\u001b[0m           g.values, self._v, g.indices)\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adam.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[0;34m(self, grad, var)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_resource_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mbeta1_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_beta_accumulators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mget_slot\u001b[0;34m(self, var, name)\u001b[0m\n\u001b[1;32m    754\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mmirrored_slot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_slot_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m_var_key\u001b[0;34m(var)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;31m# TODO(ashankar): Consolidate handling for eager and graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"op\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_id\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;34m\"\"\"The op for this variable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     raise AttributeError(\n\u001b[0;32m--> 933\u001b[0;31m         \"Tensor.op is meaningless when eager execution is enabled.\")\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "BnloSnprXOX3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show(state):\n",
        "  import time\n",
        "  mappa = np.ones((5,5,3))\n",
        "  agent = state[:2]\n",
        "  cube = state[2:4]\n",
        "  goal = state[4:]\n",
        "  mappa[int(agent[0]), int(agent[1]),:] = [0.,0.,0.]\n",
        "  mappa[int(cube[0]), int(cube[1]), :] = [1.,0.,0.]\n",
        "  mappa[int(goal[0]), int(goal[1]), :] = [0.,1.,0.]\n",
        "  clear_output(True)\n",
        "  plt.imshow(mappa)\n",
        "  plt.show()\n",
        "  time.sleep(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZV-wEP9s-JAP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize the agent.\n",
        "Visualize what the agent is doing in this interactive plotting of the environment. The agent is black, the cube is red, the goal is green."
      ]
    },
    {
      "metadata": {
        "id": "dVtF8LsfBTTS",
        "colab_type": "code",
        "outputId": "0d6a0e2b-ff9f-42bc-9de9-95c3b38c7bab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "fails, wins = 0,0\n",
        "for i in range(100):\n",
        "    env = World()\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    st = 0\n",
        "    while not done:\n",
        "        st+=1\n",
        "        if st == 30: \n",
        "            done = True\n",
        "            fails+=1\n",
        "            break\n",
        "        act = policy_net.get_action(np.array(state, dtype = np.float64))\n",
        "        act = np.array(act, dtype = np.float64)\n",
        "        state = np.array(state, dtype = np.float64)\n",
        "   #     print(\"value\", value_net(state.reshape((1,-1)), act.reshape((1,-1))))\n",
        "        act += np.random.randn(2)*0.1\n",
        "    #    print(act)\n",
        "        for a in range(len(act)):\n",
        "            if act[a] < -0.3 : act[a] = -1.\n",
        "            if act[a] > -0.3 and act[a] < 0.3: act[a] = 0.\n",
        "            if act[a] > 0.3: act[a] = 1.\n",
        "    #    print(act)\n",
        "        next_state, reward, done, _ = env.step(act)\n",
        "        if reward==1:\n",
        "            wins+=1\n",
        "    #    print(next_state)\n",
        "        env.show()\n",
        "        state = next_state\n",
        "        show(state)\n",
        "    #    print(\"REWARD\", reward, \"DONE\", done)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPQAAAD4CAYAAADb7cuFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAACIBJREFUeJzt3U+InPUdx/H3Vg/GiCIoETFUCvIV\nibSYixY0iFKt1Qrin1taSKEHBUt71ILiwWLxD9pDexBC6UUPYql6EL0oaEEFSwryPYgiNkIV0YoH\nMXF62A2kZXZmnJ1nZ/bD+5XLzpPdZ75Z8t7fM7O7v1kbjUZIyvCdZQ8gaXEMWgpi0FIQg5aCGLQU\n5NRFn3CNtUGeNj/CES7l0oWec4TP8GvHWht7cNHfthoq6BEj1sb/G7Z0TmmHGhuDl9xSEIOWghi0\nFMSgpSAGLQUxaCmIQUtBDFoKYtBSEIOWghi0FMSgpSAGLQUxaCmIQUtBDFoKYtBSkJm2IKqqR4HL\ngRFwd3e/MehUkuYydYWuqgPARd19BXAIeHzwqSTNZZZL7muAZwG6+x3g7Ko6c9CpJM1llqDPAz4+\n6fbHG8fGOsIRRgP8AQY5p5Rknm18J269ueitdk9w109pullW6KP874p8PvDRMONI2opZgn4RuBWg\nqi4Djnb3F4NOJWkuM220X1W/A64CvgHu7O5/bHpCN9qXtoOvnDHunNIO5StnSOkMWgpi0FIQg5aC\nGLQUxKClIAYtBTFoKYhBS0EMWgpi0FIQg5aCGLQUxKClIAYtBTFoKcg8mwRONOSmAW5IIE3mCi0F\nMWgpiEFLQQxaCmLQUhCDloIYtBTEoKUgBi0FMWgpiEFLQQxaCmLQUhCDloIYtBTEoKUgBi0FmSno\nqtpXVe9W1V1DDyRpflODrqrdwBPAy8OPI2krZlmhvwJuAI4OPIukLZq6SWB3HwOOVdU2jCNpK3xS\nTApi0FIQg5aCrI1Gkzevr6r9wMPAhcDXwL+AW7r7000+xN3wpeGtjT04Leg5GLQ0vLFBe8ktBTFo\nKYhBS0EMWgpi0FIQg5aCGLQUxKClIAYtBTFoKYhBS0EMWgpi0FIQg5aCGLQUxKClIFN3/dQKWRv7\nO+1bNxoNc+7Fb54xmLWBPrej0WiQc2+2MYkrtBTEoKUgBi0FMWgpiEFLQQxaCmLQUhCDloIYtBTE\noKUgBi0FMWgpiEFLQQxaCmLQUhCDloIYtBTEoKUgM21BVFUPAVduvP+D3f3MoFNJmsvUFbqqrgb2\ndfcVwPXAY4NPJWkus1xyvwLctvH2Z8DuqjpluJEkzWvqJXd3Hwe+3Lh5CHhh45i225C7aO6gHTqH\nsNkumqt47km7iM68jW9V3cx60D9awEyah9v4DmanbeO7mVmfFLsOuAe4vrs/H3YkSfNam3Y5UFVn\nAa8C13b3v2c45875srzTuEIPZqet0KPRaOxJZ1mh7wDOAZ6uqhPHDnb3BwuaTdKCTF2h57Bzvizv\nNK7Qg0lZof1JMSmIQUtBDFoKYtBSEIOWghi0FMSgpSAGLQUxaCmIQUtBDFoKYtBSEIOWghi0FMSg\npSAGLQVxgwNpZ3KDAymdQUtBDFoKYtBSEIOWghi0FMSgpSAGLQUxaCmIQUtBDFoKYtBSEIOWghi0\nFMSgpSAGLQUxaCnIqdPeoapOBw4De4DTgAe6+7mB55I0h1lW6JuAN7v7AHA78MiwI0ma19QVuruf\nOunmXuDD4caRtBVTgz6hql4DLgBuHG4cSVvxrXb9rKofAH8Gvt/dm32gu35Kw5tv18+q2l9VewG6\n+23WV/VzFzubpEWY5Umxq4DfAFTVHuAM4JMhh5I0n6mX3FW1C3iS9SfEdgH3d/ffJnyIl9zS8MZe\ncvvKGdLO5CtnSOkMWgpi0FIQg5aCGLQUxKClIAYtBTFoKYhBS0EMWgpi0FIQg5aCGLQUxKClIAYt\nBTFoKYhBS0EMWgpi0FIQg5aCGLQUxKClIAYtBTFoKYhBS0EMWgpi0FIQg5aCGLQUxKClIAYtBTFo\nKYhBS0EMWgpi0FKQmYKuql1V9W5V/XzgeSRtwawr9L3Ap0MOImnrpgZdVRcDlwDPDz+OpK2YZYV+\nGPj10INI2rqJQVfVQeD17n5vm+aRtAVro9Fo07+sqqeA7wHHgQuAr4BfdvdLE865+QklLcra2IOT\ngj5ZVd0HvN/dh6e8q0FLwxsbtN+HloLMvEJ/C67Q0vBcoaV0Bi0FMWgpiEFLQQxaCmLQUhCDloIY\ntBTEoKUgBi0FMWgpiEFLQQxaCmLQUhCDloIYtBTk1AHOOfYXryUNzxVaCmLQUhCDloIYtBTEoKUg\nBi0FMWgpyBDfh16oqnoUuJz1Dfzv7u43ljzSRFW1D/gr8Gh3/2HZ80xSVQ8BV7L+/+DB7n5mySNt\nqqpOBw4De4DTgAe6+7mlDjVFVe0C/sn6rIe34z5XeoWuqgPARd19BXAIeHzJI01UVbuBJ4CXlz3L\nNFV1NbBv43N7PfDYkkea5ibgze4+ANwOPLLkeWZxL/Dpdt7hSgcNXAM8C9Dd7wBnV9WZyx1poq+A\nG4Cjyx5kBq8At228/Rmwu6pOWeI8E3X3U9390MbNvcCHy5xnmqq6GLgEeH4773fVL7nPA9466fbH\nG8f+s5xxJuvuY8Cxqlr2KFN193Hgy42bh4AXNo6ttKp6jfWXNr5x2bNM8TBwF/Cz7bzTVV+h/58/\nJ75gVXUz60HftexZZtHdPwR+Cvylqlby/0NVHQRe7+73tvu+Vz3oo6yvyCecD3y0pFniVNV1wD3A\nj7v782XPM0lV7a+qvQDd/TbrV5fnLneqTf0EuLmq/g78AvhtVV27HXe86pfcLwL3A3+qqsuAo939\nxZJnilBVZwG/B67t7m194mZOVwHfBX5VVXuAM4BPljvSeN19x4m3q+o+4P3ufmk77nulg+7u16rq\nrY3HTd8Ady57pkmqaj/rj50uBL6uqluBW1Y0mDuAc4CnT3rMf7C7P1jeSBP9EXiyql4FdgF3dvc3\nS55p5Qzxgu+SlmTVH0NL+hYMWgpi0FIQg5aCGLQUxKClIAYtBfkvTxUG/USRpy8AAAAASUVORK5C\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f42e400c438>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-86afa4783e06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;31m#    print(\"REWARD\", reward, \"DONE\", done)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d2cb622cad5e>\u001b[0m in \u001b[0;36mshow\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmappa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "GIgtCzqxpaMR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mPtAqP5LHkpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}